---
title: "ChatGPT in biomedical reasearch"
author: "Leonidas Lundell, PhD"
date: "2023-06-26"
output:
  html_document: null
  pdf_document: default
editor_options:
  chunk_output_type: inline
---

<!-- toc: true -->

<!--     toc_depth: 2 -->

<!--     toc_float: false -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      fig.align = 'center',
                      message = FALSE,
                      warning = FALSE)
library(ggplot2)
library(tidyverse)
library(scales)
library(data.table)

theme_set(
  theme_linedraw() +
    theme(axis.text = element_text(size = 14)) +
    theme(axis.title = element_text(size = 16)) +
    theme(plot.title = element_text(size = 20))
  )

wd <- Sys.getenv("wd")
data <- paste0(wd, "/data/")

load(paste0(data, "GPT_response.Rdata"))

diseases <- fread(paste0(data, "human_disease_textmining_filtered.tsv"))[,1:4]
colnames(diseases) <- c("identifier", "symbol", "DO_ID", "disease")

well_known$knowledge <- "well known"
intermediate_known$knowledge <- "intermediate known"

results <- rbind(intermediate_known, well_known)
results$gene_hits <- as.numeric(results$gene_hits)
results$ref_correct <- as.numeric(results$ref_correct)
results$correct <- as.factor(results$correct)
```

# Introduction

ChatGPT and LLMs in general have taken the world by storm. A lot of attention has been given to what you can and cannot do with it, both analytically and opinion-based. I got curious to know whether ChatGPT can be helpful in a topic that I know well: molecular biology and genetics.

[OpenAIs](https://arxiv.org/pdf/2303.08774.pdf) own published analysis suggests that ChatGPT performs as if it is in the top 4/5ths of students taking the Advanced Placement (AP) biology exam, and scores 53% on the Medical Knowledge Self-Assessment Program. These observations have been generally replicated in various other contexts, including descriptions of medical [conditions](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10002821/) and tests given to medical [students](https://pubmed.ncbi.nlm.nih.gov/36753318/).

Together, this suggest that ChatGPT might be useful tool for biomedical research. However, there's the looming question of its tendency to "hallucinate" or generate information that isn't accurate. I set out to understand whether chatGPT would be useful as an assistant or database to understand gene disease relationships.

While I have a definitive answer on whether chatGPT hallucinates, I think that the most important insight is the value of context.

In the analysis below, I analyze:

-   The performance of chatGPT 3.5 and 4 with gene disease associations of *well known genes*

-   The performance of chatGPT 3.5 and 4 with gene disease associations of *less known genes*

-   The effect of context in the accuracy of chatGPT 4

-   And the accuracy of generated references and follow up material

# chatGPT as a biomedical assistant

# Assumptions

One, almost philosophical question, is how to define correct answers for a question like this.

My basic premise, is that if a gene disease assocaitions is present in a scientific publications it is true. Imperfect, I know. I am also assuming that a LLM does not have the ability to draw conclusions from information that does not exist. Moreover, I am not counting false negatives, i.e. chatGPT not mentioning a "known" gene disease association. I am only considering accurarcy or false positives, i.e chatGPT is proposing a gene disease association that is not found in the litterature.

I decided to do this because

# Data sources

I am basing a large part of this analysis on the brilliant and continoually updated [text mining resource](https://diseases.jensenlab.org) from the [Jensen lab](https://jensenlab.org/people/larsjuhljensen/) in Copenhagen University. I choose to use text mining as opposed to a e.g. [SNP database](https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism) as this would be more plausibly aligned to chatGPTs data ingestion, and also because SNPs are only one way of which genes affect disease.

Literature accucary and gene information was queried on NIHs PubMed database using an API.

In order to to understand the effect of prior knowledge on accuracy, I selected either the 5% most frequent diseases and top 5% of disease associated genes, or the 20%-25% most frequent diseases with corresponding 20-25% disease associated genes. From the selected genes I then randomly sampled 100 to analyze.

This approach also generates a multiple choice selection to choose from, giving me an indication to the effect of context. The selected **top 5% genes (well known genes from now on)** had around `r round(diseases[symbol %in% rownames(well_known), n_distinct(disease)], -2)` associated diseases, and the **20-25% genes (less known genes from now on)** had `r round(diseases[symbol %in% rownames(intermediate_known), n_distinct(disease)], -2)` associated diseases.

Below you can see the number of PubMed mentions for the randomly selected genes:

```{r}
x_labels <- scale_x_discrete(labels = c("Less known genes", "Well known genes"))
  
results |> 
  filter(model == "GPT-3.5") |> 
  ggplot() +
  geom_boxplot(
    mapping = aes(x = knowledge, y = gene_hits, fill = knowledge),
    color = "black") +
  x_labels +
  scale_y_continuous(trans = log10_trans(),
                     breaks = trans_breaks("log10", function(x) 10^x),
                     labels = trans_format("log10", math_format(10^.x))) +
  scale_fill_viridis_d(option = "H") +
  theme(legend.position = "none") +
  labs(y = "Number of mentions in literature", x = "")
```

The average number of literature mentions for the well known genes was around `r results[results$model == "GPT-3.5" & results$knowledge == "well known", "gene_hits"] |> mean() |> round(digits = -2)` and around `r results[results$model == "GPT-3.5" & results$knowledge == "intermediate known", "gene_hits"] |> mean() |> round(digits = -2)` for the less known genes.

# chatGPT accuracy in gene disease associations

For each of the selected genes I querried chatGPT 3.5 and chatGPT 4 with the follwing prompt:

"Answer as an expert medical doctor, molecular biologist, and geneticist. Which disease from the diseaselist below is gene GENESYMBOL involved in? Reply only with the name of the disease. Diseaselist:", substituting GENESYMBOL and the providing the a list of diseases as appropriate.

I defined correct responses as responses where at least one of the suggested diseases were associated with a disease, increasing the apparent accuracy of chatGPT.

```{r}
results |> 
  group_by(knowledge, model) |> 
  summarise(
    accuracy = sum(as.logical(correct))/length(correct),
  ) |> 
  ggplot() +
  geom_bar(mapping = aes(x = knowledge, y = accuracy, fill = model), 
           stat = "identity", position = "dodge", color = "black") +
  scale_fill_viridis_d() +
  scale_y_continuous(breaks = seq(0, 1, .25),
                     labels = paste0(seq(0, 100, 25), "%"),
                     limits = c(0,1), 
                     expand = c(0,0)) +
  x_labels +
  labs(x = "", y = "chatGPT accuracy")
```

Intriguingly, chatGPT 4 performs stunningly good for well known genes, while the currenlty free version of chatGPT suggests at least one correct gene disease association 50% of the times. For less known genes, neither model performs usefully at all.

# Context has a major effect on accuracy

To better understand the effect of context as a provided disease list, I queried chatGPT 4 with the only well known genes. I then manually evaluated the responses against the gene disease database, giving chatGPT the benefit of the doubt. As examples, I scored metabolic syndrome as a correct response to T2D, and rheumatic disease with rheumatic arthitis. I suppose I could have asked GPT to do this, but I rather retain control, and I would have manually validated in either case. Overall, the results are not very encouraging:

```{r}
well_known_freeform <- cbind(well_known[101:200,], 
                             freeform = well_known_freeform) |> 
  summarise(
    with_context = sum(as.logical(correct))/length(correct),
    no_context = sum(as.logical(freeform))/length(freeform),
  ) |> 
  reshape::melt() 

ggplot(well_known_freeform) +
  geom_bar(mapping = aes(x = variable, y = value, fill = variable), 
           stat = "identity", position = "dodge", color = "black") +
  scale_fill_viridis_d(option = "E") +
  scale_x_discrete(labels = c("No context", "With disease list"), limits = c("no_context", "with_context")) +
  scale_y_continuous(breaks = seq(0, 1, .25),
                     labels = paste0(seq(0, 100, 25), "%"),
                     limits = c(0,1), 
                     expand = c(0,0)) +
  theme(legend.position = "none") +
  labs(x = "", y = "chatGPT accuracy")
```

Without context, chatGPT 4 performs around 30% worse than with context, and only 15% better than chatGPT 3.5 with context.

This benchmark is in my opinion the most important of this whole analysis. It is quite unrealistic that one would have a disease list to compare to in real life. Moreover, LLMs in general do not differentiate between suggestions and context, everything provided is a context regardless of how we consider it. By providing the most common diseases as a list to choose from, we are implicitly providing a context.

It would be intresting to see if the prompt did not include any instruction about the list of diseases but simply stated a number of diseases.

# You should really not be using chatGPT for references

It is well-understood ([by some](https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/)) that chatGPT hallucinates [references](https://blog.smu.edu/smulibraries/2023/01/20/artificial-intelligence-and-the-research-paper-a-librarians-perspective/)/[citations](https://www.cureus.com/articles/138667-artificial-hallucinations-in-ChatGPT-implications-in-scientific-writing#!/). To better understand how chatGPT behaves in the biomedical context, prompted it to:

"Give me 5 or fewer scientific literature references to further my understanding of gene GENESYMBOL. Answer only with article title. Do not elaborate. Do not include authors. Separate by new line. Text:"

```{r}
results$ref_correct[results$ref_correct == (results$ref_correct |>  unique())[5]] <- 0.2
results |> 
  group_by(knowledge, model) |> 
  count(ref_correct) |> 
  transform(n = n/100) |> 
  ggplot() +
  geom_bar(mapping = aes(x = ref_correct, y = n, fill = model), 
           stat = "identity", position = "dodge", width = 0.15) +
  scale_fill_viridis_d() +
  scale_y_continuous(breaks = seq(0, 1, .25),
                     labels = paste0(seq(0, 100, 25), "%"),
                     limits = c(0,1),
                     expand = c(0,0)) +
  scale_x_continuous(breaks = seq(0, .6, .2),
                     labels = paste0(seq(0, 60, 20), "%"),
                     limits = c(-0.1,.7)) +
  facet_wrap(~knowledge,
             labeller=as_labeller(
               c("intermediate known" = "less well known",
                           "well known" = "well known")
             ))+
  theme(strip.background = element_rect("white"),
        strip.text = element_text(size = 14, color = "black")) +
  labs(x = "Percentage of chatGPT references\nexisting in PubMed database",
       y = "Percentage of responses")
```

As you can see, chatGPT hallucinates references in the vast majority of cases, with over 75% of suggested references not existing in PubMed regardless of prior knowledge or model used.

In addiontion to the poor accuracy of the responses, my personal opinion is that using chatGPT to find references is inappropriate simply because there are better tools to do so.

# Conclusion

## Manual inspection of data

```{r}
# paste0(data, "GPT_manual_review_response.Rdata")
# 
# set.seed(42)
# well_known_manual <- sample(names(well_known_genes_4), 5)
# intermediate_known_manual <- sample(names(intermediate_known_genes_3.5), 5)
# 
# sapply(well_known_genes_3.5[well_known_manual],\(x) x$disease)
# sapply(well_known_genes_4[well_known_manual],\(x) x$disease)
# sapply(intermediate_known_genes_3.5[intermediate_known_manual],\(x) x$disease)
# sapply(intermediate_known_genes_4[intermediate_known_manual],\(x) x$disease)

```

```{r}
# manual_inspection <- lapply(names(GPT_response), \(gene){
#   x <- GPT_response[[gene]]
#   list(predicted_disease = x$gpt_out$disease,
#        true_disease = x$disease,
#        gene = gene)
# })
# 
# names(manual_inspection) <- names(GPT_response)
# 
# manual_inspection <- manual_inspection |>
#   reshape2::melt(id.vars = c("predicted_disease", "true_disease", "gene"))
# 
# colnames(manual_inspection) <- c("Gene","Actual disease", "GPT predicted disease")
# 
# DT::datatable(manual_inspection,
#               options = list(
#                 lengthMenu = list(c(25, 50, -1),
#                                   c('25', '50', 'All')),
#                 paging = T))
```

# Code availability

All code and data is available on <https://github.com/leonidaslundell/GPT_biomedical_accuracy>.
