---
title: "ChatGPT as an assitant to biomedicine and genes"
author: "Leonidas Lundell, PhD"
date: "2023-06-26"
output:
  html_document: null
  pdf_document: default
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      fig.align = 'center',
                      message = FALSE,
                      warning = FALSE)
library(ggplot2)
library(tidyverse)
library(scales)
library(data.table)

theme_set(
  theme_linedraw() +
    theme(axis.text = element_text(size = 14)) +
    theme(axis.title = element_text(size = 16)) +
    theme(plot.title = element_text(size = 20))
  )

wd <- Sys.getenv("wd")
data <- paste0(wd, "/data/")

load(paste0(data, "GPT_response.Rdata"))
load(paste0(data, "GPT_manual_review_response.Rdata"))

diseases <- fread(paste0(data, "human_disease_textmining_filtered.tsv"))[,1:4]
colnames(diseases) <- c("identifier", "symbol", "DO_ID", "disease")

well_known$knowledge <- "well known"
intermediate_known$knowledge <- "intermediate known"

results <- rbind(intermediate_known, well_known)
results$gene_hits <- as.numeric(results$gene_hits)
results$ref_correct <- as.numeric(results$ref_correct)
results$correct <- as.factor(results$correct)
```

# Introduction

ChatGPT and LLMs (Large Language Models) in general have taken the world by storm. A lot of attention has been given to what you can and cannot do with it, both analytically and opinion-based. I became curious about whether ChatGPT can be helpful in a topic that I know well: molecular biology and genetics.

[OpenAI's](https://arxiv.org/pdf/2303.08774.pdf) own published analysis suggests that ChatGPT performs as if it is in the top 4/5ths of students taking the Advanced Placement (AP) biology exam and scores 53% on the Medical Knowledge Self-Assessment Program. These observations have been generally replicated in various other contexts, including descriptions of medical [conditions](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10002821/) and tests given to medical [students](https://pubmed.ncbi.nlm.nih.gov/36753318/).

This suggests that ChatGPT might be a useful tool for biomedical research. However, there\'s the looming question of its tendency to \"hallucinate\" or generate information that isn\'t accurate. I set out to determine whether ChatGPT would be useful as an assistant or database to understand gene-disease relationships.

While I have answered the question of ChatGPT gene-disease hallucinations, I believe that the most crucial insight in this analysis lies in the value of prompt context.

In the analysis below, I examine:

-   The performance of ChatGPT 3.5 and 4 with gene-disease associations of well-known genes

-   The performance of ChatGPT 3.5 and 4 with gene-disease associations of less known genes

-   The effect of context on the accuracy of ChatGPT 4

-   The accuracy of generated references.

# Assumptions

A significant question, almost philosophical in nature, is how to define correct answers for such an analysis.

My basic premise is that if a gene-disease association is present in scientific publications, it is true. This is an imperfect assumption. Furthermore, I'm assuming that an LLM does not have the capability to infer information that doesn't exist. Additionally, I'm not counting false negatives, i.e., ChatGPT not mentioning a \"known\" gene-disease association. I am focusing only on accuracy or false positives, where ChatGPT proposes a gene-disease association not found in the literature. I also decided to include explicit false negatives, ie. ChatGPT stating that there are no gene-disease associations, as a false positive (statement).

I put the emphasis on false gene-disease associations because they hold the potential to misguide and confuse. Overlooking a recognized association is, of course, a shortfall. However, the proposition of an inaccurate association by ChatGPT could be considerably more detrimental, potentially directing researchers towards unproductive or misguided avenues.

# Data sources

I base a significant part of this analysis on the continuously updated [text mining resource](https://diseases.jensenlab.org) from the [Jensen lab](https://jensenlab.org/people/larsjuhljensen/) in Copenhagen University. I chose text mining over, for instance, an [SNP database](https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism) because it aligns more closely with ChatGPT's data ingestion. Furthermore, SNPs represent just one way genes can affect disease.

Literature accuracy and gene information were queried on NIH's PubMed database using an API.

To understand the effect of prior knowledge on accuracy, I selected either the top 5% of diseases and the top 5% of disease-associated genes, or the 20-25% most frequent diseases with the corresponding 20-25% disease-associated genes. I then randomly sampled 100 genes from this selection for analysis.

This method also provides a multiple-choice selection to consider, offering insight into the effect of context. The top 5% genes (referred to as "well-known genes") had approximately `r round(diseases[symbol %in% rownames(well_known), n_distinct(disease)], -2)` associated diseases, and the 20-25% genes (referred to as "less known genes") had `r round(diseases[symbol %in% rownames(intermediate_known), n_distinct(disease)], -2)` associated diseases. This list of diseases was provided for ChatGPT to choose from (unless otherwise specified).

Below you can see the number of PubMed mentions for the randomly selected genes:

```{r}
x_labels <- scale_x_discrete(labels = c("Less known genes", "Well known genes"))
  
results |> 
  filter(model == "GPT-3.5") |> 
  ggplot() +
  geom_boxplot(
    mapping = aes(x = knowledge, y = gene_hits, fill = knowledge),
    color = "darkgray") +
  x_labels +
  scale_y_continuous(trans = log10_trans(),
                     breaks = trans_breaks("log10", function(x) 10^x),
                     labels = trans_format("log10", math_format(10^.x))) +
  scale_fill_viridis_d(option = "H") +
  theme(legend.position = "none") +
  labs(y = "Number of mentions in literature", x = "")
```

The average literature mentions for well-known genes was around `r results[results$model == "GPT-3.5" & results$knowledge == "well known", "gene_hits"] |> mean() |> round(digits = -2)` and around `r results[results$model == "GPT-3.5" & results$knowledge == "intermediate known", "gene_hits"] |> mean() |> round(digits = -2)` for the less-known genes.

# chatGPT accuracy in gene disease associations

For each selected gene, I queried ChatGPT 3.5 and ChatGPT 4 with the following prompt:

"Answer as an expert medical doctor, molecular biologist, and geneticist. Which disease from the diseaselist below is gene GENESYMBOL involved in? Reply only with the name of the disease. Diseaselist:", substituting GENESYMBOL and providing a list of diseases as appropriate.

I defined correct responses as those where at least one of the proposed diseases was associated with the gene, thereby potentially increasing ChatGPT's apparent accuracy.

```{r}
results |> 
  group_by(knowledge, model) |> 
  summarise(
    accuracy = sum(as.logical(correct))/length(correct),
  ) |> 
  ggplot() +
  geom_bar(mapping = aes(x = knowledge, y = accuracy, fill = model), 
           stat = "identity", position = "dodge", color = "black") +
  scale_fill_viridis_d() +
  scale_y_continuous(breaks = seq(0, 1, .25),
                     labels = paste0(seq(0, 100, 25), "%"),
                     limits = c(0,1), 
                     expand = c(0,0)) +
  x_labels +
  labs(x = "", y = "chatGPT accuracy")
```

Intriguingly, ChatGPT 4 performs exceptionally well for well-known genes, while the currently free version of ChatGPT suggests incorrect gene-disease association 50% of the time. For less-known genes, neither model performs satisfactorily.

A specific case of this analysis results from ChatGPT claiming that there are no known gene-disease associations. While this technically constitutes a false negative, I feel that it is quallitatively different from omiting a disease, the fact that it is explicitly stated has the potential mislead.

```{r}
explicit_negatives <- c(
  'GPT-3.5' = sapply(intermediate_known_genes_3.5,\(x){
  grepl("involv|associat|implic", x$disease)
  }) |> table() |> prop.table() |> last(),
  'GPT-4' = sapply(intermediate_known_genes_4,\(x){
    grepl("involv|associat|implic", x$disease)
    }) |> table() |> prop.table() |> last()
  )

ggplot() +
  geom_bar(aes(y = explicit_negatives, 
               x = names(explicit_negatives),
               fill = names(explicit_negatives)),
           stat = "identity", color = "black") +
  scale_fill_viridis_d() +
  scale_y_continuous(breaks = seq(0, 1, .25),
                     labels = paste0(seq(0, 100, 25), "%"),
                     limits = c(0,1), 
                     expand = c(0,0)) +
  theme(legend.position = "none") +
  labs(x = "", y = "chatGPT accuracy")

```

For the well known genes, ChatGPT 3.5 claimed that 4 genes explicitly did not have any associations with diseases, when indeed there are known gene-disease associations in the literature. ChatGPT 4 on the hand provided gene-disease associations for all investigated genes.

For the less well studied genes shown in the figure above, ChatGPT 4 is more conservative than ChatGPT 3.5 by suggesting more often that there is no associations.

# Context has a major effect on accuracy

To delve deeper into context's impact on a provided disease list, I queried ChatGPT 4 using only well-known genes. I manually evaluated the responses against the gene-disease database, giving ChatGPT the benefit of the doubt. For instance, I considered "metabolic syndrome" as a correct response to "T2D" and "rheumatic disease" as correct for "rheumatic arthritis." While I could have tasked ChatGPT with this, I preferred to retain control, as manual validation would have been necessary regardless. Overall, the results are not great:

```{r}
well_known_freeform <- cbind(well_known[101:200,], 
                             freeform = well_known_freeform) |> 
  summarise(
    with_context = sum(as.logical(correct))/length(correct),
    no_context = sum(as.logical(freeform))/length(freeform),
  ) |> 
  reshape::melt() 

ggplot(well_known_freeform) +
  geom_bar(mapping = aes(x = variable, y = value, fill = variable), 
           stat = "identity", position = "dodge", color = "black") +
  scale_fill_viridis_d(option = "E") +
  scale_x_discrete(labels = c("No context", "With disease list"), limits = c("no_context", "with_context")) +
  scale_y_continuous(breaks = seq(0, 1, .25),
                     labels = paste0(seq(0, 100, 25), "%"),
                     limits = c(0,1), 
                     expand = c(0,0)) +
  theme(legend.position = "none") +
  labs(x = "", y = "chatGPT accuracy")
```

Without context, ChatGPT 4 performs about 30% worse than with context, and only 15% better than ChatGPT 3.5 when context is provided.

I believe this benchmark is the most telling part of this entire analysis. Realistically, one might not have a disease list for real-life comparison. Furthermore, LLMs do not differentiate between disease lists and context; to them, everything is context. By providing common diseases as a list, we're implicitly offering context.

It would be interesting to explore if the prompt did not include any instructions about the disease list but merely listed several diseases.

# You should really not be using chatGPT for references

It is well-understood ([by some](https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/)) that chatGPT hallucinates [references](https://blog.smu.edu/smulibraries/2023/01/20/artificial-intelligence-and-the-research-paper-a-librarians-perspective/)/[citations](https://www.cureus.com/articles/138667-artificial-hallucinations-in-ChatGPT-implications-in-scientific-writing#!/). To better understand how chatGPT behaves in the biomedical context, prompted it to:

"Give me 5 or fewer scientific literature references to further my understanding of gene GENESYMBOL. Answer only with article title. Do not elaborate. Do not include authors. Separate by new line. Text:"

```{r}
results$ref_correct[results$ref_correct == (results$ref_correct |>  unique())[5]] <- 0.2
results |> 
  group_by(knowledge, model) |> 
  count(ref_correct) |> 
  transform(n = n/100) |> 
  ggplot() +
  geom_bar(mapping = aes(x = ref_correct, y = n, fill = model), 
           stat = "identity", position = "dodge", width = 0.15) +
  scale_fill_viridis_d() +
  scale_y_continuous(breaks = seq(0, 1, .25),
                     labels = paste0(seq(0, 100, 25), "%"),
                     limits = c(0,1),
                     expand = c(0,0)) +
  scale_x_continuous(breaks = seq(0, .6, .2),
                     labels = paste0(seq(0, 60, 20), "%"),
                     limits = c(-0.1,.7)) +
  facet_wrap(~knowledge,
             labeller=as_labeller(
               c("intermediate known" = "less well known",
                           "well known" = "well known")
             ))+
  theme(strip.background = element_rect("white"),
        strip.text = element_text(size = 14, color = "black")) +
  labs(x = "Percentage of chatGPT references\nexisting in PubMed database",
       y = "Percentage of responses")
```

Regrettably, ChatGPT often fabricated references, with over 75% of suggested references not existing in PubMed, irrespective of prior knowledge or the model used.

Beyond the concerning accuracy, I believe that utilizing ChatGPT to locate references is unsuitable, especially when more efficient tools are available.

# Conclusion

In some settings, ChatGPT is very impressive. In other contexts, ChatGPT is not useful at all. Recognizing that prompt engineering and context matters is of utmost importance. And, as in any tool, recognizing how and when to use it of outmost importance. While promising, I would be cautious in using ChatGPT for understanding gene-disease associations and genes in general.

It would be very interesting to run the same tests on biomedicaly trained LLMs such as [BioBERT](https://arxiv.org/pdf/1901.08746.pdf) and [BioGPT](https://academic.oup.com/bib/article/23/6/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9&login=false), but unfortunately I do not have access to that inference. BioGPT would be especially interesting, as it is trained on 15M abstracts, and shows very promising results.

Finally, as ChatGPT is the main interface for AI for most people, recognizing and discussing its shortcomings is especially important.

```{r}
#fix the padded number at the end of the results
intermediate_known$symbol <- rownames(intermediate_known)
intermediate_known$symbol[101:200] <- 
  str_sub(intermediate_known$symbol[101:200],
          end = -2)
well_known$symbol <- rownames(well_known)
well_known$symbol[101:200] <- 
  str_sub(well_known$symbol[101:200],
          end = -2)

```

```{r}
#subset the explicit negative
subset_negatives <- function(genes){
  neg <- sapply(genes,\(x){
    grepl("involv|associat|implic|list", x$disease)
    })
  genes <- names(genes)[neg]
  return(genes)
}

set.seed(1)
explicit_neg_inter_3.5 <- subset_negatives(intermediate_known_genes_3.5) |> sample(size = 5)
explicit_neg_inter_4 <- subset_negatives(intermediate_known_genes_4) |> sample(size = 5)

#show 5 genes with explicit negative predictions, together with the disease associations

explicit_neg_inter_3.5 <- diseases[symbol %in% explicit_neg_inter_3.5] |>
    group_by(symbol) |> 
    summarise(test = toString(disease)) |>
    ungroup()

explicit_neg_inter_4 <- diseases[symbol %in% explicit_neg_inter_4] |>
    group_by(symbol) |> 
    summarise(test = toString(disease)) |>
    ungroup()

#show 5 genes with wrong predictions, together with the disease associations
subset_error <- function(table, gpt, model, diseases=diseases){
  
  positive_prediction <- sapply(gpt,\(x){
    !grepl("involv|associat|implic|list", x$disease)
    })
  
  demo <- table[!as.logical(table$correct) & positive_prediction,]
  demo <- demo[demo$model == model, "symbol"]# |> sample(5)
  
  demo_gpt <- sapply(gpt[demo],\(x){
    x$disease
    })
  
  demo_true <- diseases[symbol %in% demo] |> 
    group_by(symbol)|> 
    summarise(test = toString(sort(disease))) |> 
    ungroup()
  
  demo_true <- demo_true[match(demo, demo_true$symbol),]
  
  demo <- data.frame(symbol = demo, 
                     GPT_predicted_disease = demo_gpt, 
                     database_associated_disease = demo_true$test,
                     row.names = NULL)
  return(demo)
}

well_known_demo_3.5 <- subset_error(well_known, 
                                well_known_genes_3.5, 
                                "GPT-3.5",
                                diseases)

well_known_demo_4 <- subset_error(well_known, 
                                well_known_genes_4,
                                "GPT-4",
                                diseases)

less_well_known_demo_3.5 <- subset_error(intermediate_known, 
                                intermediate_known_genes_3.5, 
                                "GPT-3.5",
                                diseases)


less_well_known_demo_4 <- subset_error(intermediate_known, 
                                intermediate_known_genes_4,
                                "GPT-4",
                                diseases)
# View(less_well_known_demo_3.5)
# View(less_well_known_demo_4)
# View(well_known_demo_3.5)
# View(well_known_demo_4)
```

# Manual inspection of results

```{r}
DT::datatable(less_well_known_demo_3.5, 
              caption = htmltools::tags$caption('Less well known genes predictions in GPT 3.5') )
DT::datatable(less_well_known_demo_4,
              caption = htmltools::tags$caption('Less well known genes predictions in GPT 4'))
DT::datatable(well_known_demo_3.5,
              caption = htmltools::tags$caption('Well known genes predictions in GPT 3.5'))
DT::datatable(well_known_demo_4,
              caption = htmltools::tags$caption('Well known genes predictions in GPT 4'))

```

# Code availability

All code and data is available on <https://github.com/leonidaslundell/GPT_biomedical_accuracy>.
